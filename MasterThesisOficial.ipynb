{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1ba1167a-871b-4d86-974f-1a04d0243f3f",
   "metadata": {},
   "source": [
    "# Ludwig Maximilians Universitaet Muenchen Master Thesis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0b1c7e9-a8ba-48ea-a46c-21dda7c0c2da",
   "metadata": {},
   "source": [
    "## Study Population\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9c73815-69e0-45ac-8225-03378d6e40db",
   "metadata": {},
   "source": [
    "###### Section Objective: Achieve a Data Frame for the study population that contains: the Patient ID (PID), the date of first operation (First_OP), whether there was a revision (1_Revision), the date of the revision (Revision_Datum), the begin of the observation period which is set to 2 year (Begin_BasePeriod) and the end of the observation period (End_BasePeriod). Additionally, for the LSTM Dataframe multiple lines per patient are available, as the observation period is split in subperiods, each with the begin (PeriodStart) and end (PeriodEnd) of subperiod and a subperiod identifier (PeriodNumber). Finally, the sex of the patient is tagged and the age of the patient at the moment of the start of the subperiod is also tagged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2610dbeb-cf60-404a-ad6c-d4c8e887d6e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Library import\n",
    "                                                     \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime, timedelta\n",
    "import sys\n",
    "sys.path.insert(0, '../base_code/Config_Files')\n",
    "sys.path.insert(0, '../base_code/classes')\n",
    "import importlib\n",
    "from Config_V2 import *\n",
    "from Config_V3 import *\n",
    "import Config_V2\n",
    "import Config_V3\n",
    "from PatientTimeProfiles import *\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "import shap\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import roc_curve, auc, confusion_matrix, roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from skopt import BayesSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import keras_tuner as kt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from keras_tuner.tuners import BayesianOptimization\n",
    "from tensorflow.keras.layers import Bidirectional\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.layers import Conv1D\n",
    "from sklearn.utils import class_weight\n",
    "import random\n",
    "import optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02b45188-6ee9-4ab7-907d-1e562d78592f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Seed setting\n",
    "def set_seed(seed_value=42):\n",
    "    \"\"\"Fixes random seed for reproducibility.\"\"\"\n",
    "    random.seed(seed_value)\n",
    "    np.random.seed(seed_value)\n",
    "    tf.random.set_seed(seed_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e96b79ba-663a-4136-9afe-3d98826cc905",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data reading into StudyPopulation\n",
    "StudyPopulation = pd.read_csv(data_path + new_version_path + \"StudyPopulation_v24_08.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cf0ee16-c0b8-498f-a5fe-de809e09bb77",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Begin and End Base Period columns creation\n",
    "StudyPopulation['First_OP'] = pd.to_datetime(StudyPopulation['First_OP'], format='%Y-%m-%d')\n",
    "StudyPopulation['Begin_BasePeriod'] = StudyPopulation['First_OP'] + timedelta(days=1)\n",
    "StudyPopulation['End_BasePeriod'] = StudyPopulation['First_OP'] + timedelta(days=ObservationDays)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec8a1895-8c00-404c-86b8-9eb6f8190519",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Function to create DataFrame with multiple lines per PID breaking the Base Period in months or quarters\n",
    "\n",
    "def create_timeframe_StudyPopulation(StudyPopulation, TimeResolution):\n",
    "    # Determine the increment days based on TimeResolution\n",
    "    match TimeResolution:\n",
    "        case \"Months\": \n",
    "            increment_days =  29\n",
    "        case \"Quarters\": \n",
    "            increment_days = 89\n",
    "\n",
    "    #Fixes the last date where data is available, so subperiods stop being generated where there is no data\n",
    "    max_end_date = pd.Timestamp(MaxInfoDate)\n",
    "    adjusted_end_dates = StudyPopulation['End_BasePeriod'].apply(lambda x: min(x, max_end_date))\n",
    "\n",
    "\n",
    "    # Calculate the total number of periods each original row will be split into. Final plus 1 is since the use of the floor division and that\n",
    "    # the difference between Begin and End Base Period has 1 day less than exact\n",
    "    num_periods = ((adjusted_end_dates - StudyPopulation['Begin_BasePeriod']).dt.days // (increment_days + 1)) + 1\n",
    "    \n",
    "    # Create arrays for PeriodStart and PeriodEnd\n",
    "    period_starts = []\n",
    "    period_ends = []\n",
    "    row_ids = []\n",
    "    period_numbers = []\n",
    "\n",
    "    #zip creates lines each with an element from each involved column. Takes a begin and end from a base period and runs for as many times as num \n",
    "    #periods there are, setting the start and end of subperiods, as well as the count of subperiods created\n",
    "    for i, (start_date, end_date, periods) in enumerate(zip(StudyPopulation['Begin_BasePeriod'], adjusted_end_dates, num_periods)):\n",
    "        for j in range(periods):\n",
    "            current_start = start_date + pd.Timedelta(days=j * (increment_days + 1))\n",
    "            current_end = min(current_start + pd.Timedelta(days=increment_days), end_date)\n",
    "            \n",
    "            #Saves start and end and number of subperiod\n",
    "            period_starts.append(current_start)\n",
    "            period_ends.append(current_end)\n",
    "            row_ids.append(i)\n",
    "            period_numbers.append(j + 1)\n",
    "\n",
    "    # Create the expanded DataFrame\n",
    "    expanded_df = pd.DataFrame({\n",
    "        'PeriodStart': period_starts,\n",
    "        'PeriodEnd': period_ends,\n",
    "        'RowID': row_ids,\n",
    "        'PeriodNumber': period_numbers\n",
    "    })\n",
    "\n",
    "    # Merge with the original StudyPopulation DataFrame on the RowID and then drops\n",
    "    IStudyPopulation = expanded_df.merge(StudyPopulation, left_on='RowID', right_index=True).drop(columns=['RowID'])\n",
    "\n",
    "    return IStudyPopulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e03f716-1174-4130-8227-5c88a84a831d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Calling of the function providing a particular DataFrame and time frame\n",
    "#date of revision and first operation date missing\n",
    "IStudyPopulation = create_timeframe_StudyPopulation(StudyPopulation, TimeResolution)\n",
    "IStudyPopulation = IStudyPopulation[[\"PID\",\"1_Revision\",\"PeriodNumber\", \"PeriodStart\", \"PeriodEnd\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90e9b92f-7a9e-43e7-99ea-b9fa877218ce",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "increment_days"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5654092-9d2e-4a57-a15c-54e7f140be01",
   "metadata": {},
   "source": [
    "## Age and Sex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d726daa5-406b-45ef-a863-9720eead8aea",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Age and Sex Data reading\n",
    "AgeAndSex = pd.read_csv(data_path + new_version_path + \"VERS_STAMM.csv\", sep=\";\")\n",
    "AgeAndSex = AgeAndSex[[\"PID\",\"Geburtsdatum\",\"Geschlecht\"]]\n",
    "AgeAndSex = AgeAndSex.drop_duplicates(subset=['PID'])\n",
    "AgeAndSex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56b2d4a0-2412-418f-816c-e37b1164de3d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Prepare the Age and Sex columns\n",
    "# Step 1: Extract the year from 'Geburtsdatum'\n",
    "AgeAndSex['Geburtsdatum'] = pd.to_datetime(AgeAndSex['Geburtsdatum'], format='%d%b%Y').dt.year\n",
    "\n",
    "# Step 2: Merge AgeAndSex with IStudyPopulation on PID\n",
    "IStudyPopulationAS = IStudyPopulation.merge(AgeAndSex[['PID', 'Geburtsdatum', 'Geschlecht']], on='PID', how='left')\n",
    "\n",
    "# Step 3: Calculate Age as the difference between the year of 'PeriodStart' and 'YearOfBirth'\n",
    "IStudyPopulationAS['PeriodStart'] = pd.to_datetime(IStudyPopulationAS['PeriodStart'], format='%Y-%m-%d')\n",
    "IStudyPopulationAS['Age'] = IStudyPopulationAS['PeriodStart'].dt.year - IStudyPopulationAS['Geburtsdatum']\n",
    "\n",
    "# Rename Geschlecht to Sex and drop YearOfBirth if not needed\n",
    "IStudyPopulationAS.rename(columns={'Geschlecht': 'Sex'}, inplace=True)\n",
    "IStudyPopulationAS.drop(columns=['Geburtsdatum'], inplace=True)\n",
    "\n",
    "# Display the final DataFrame\n",
    "print(IStudyPopulationAS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9029e181-560c-46e4-bb99-65f3e0e02297",
   "metadata": {},
   "source": [
    "## Profiling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7f69fe9-349e-4e20-a69d-7d6e93adadc1",
   "metadata": {},
   "source": [
    "###### Section Objective: A DataFrame is built to achieve a matrix structure where rows represent (PID, subperiod number after operation), columns represent for example, ATC_Codes (Medicine code) and 1s are stock whenever there is an existing medicine assigned to the patient in the subperiod, 0s shall be in the full matrix whenever it was not assigned during the subperiod for him/her. The same is done for Treatments and for Diagnostics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfce1f45-dfff-4b2a-8b21-f87fa6fce87f",
   "metadata": {},
   "source": [
    "## Prescription part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ace620e9-f96c-48d4-8b3a-065741a4c31d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Data reading into and truncates ATC_Codes\n",
    "#PZN means \"Pharmazentralnummer\" or Central number of medicine\n",
    "#FG means \"Faktorgruppe oder Fachgruppe\"\n",
    "Prescriptions = pd.read_csv(data_path + new_version_path + \"AM_EVO.csv\", sep=\";\")\n",
    "Prescriptions[\"ATCX\"] = Prescriptions[\"ATC_Code\"].str[:ATCX]\n",
    "Prescriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6dd49ea-855c-482b-bd3f-01307f277bbd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Stay just with the desired columns\n",
    "PrescriptionsF = Prescriptions[[\"PID\",\"Verordnungsdatum\",\"Anzahl_Packungen\",\"ATCX\"]]#\"ATC_Presence\"]]\n",
    "PrescriptionsF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f48862d2-4805-4d99-8708-68954c3e074c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#CREATING A DATAFRAME WITH ATCX AND MENGE AND DDD_JE_PACKUNG FOR EACH ATCX VALUE, SO VALUES CAN BE BROUGHT FOR QUANTITY OF SUBSTANCES CALCULATION\n",
    "\n",
    "some_df = pd.read_csv(data_path_DZO + new_version_path + \"AM_EVO_DDD.csv\", delimiter= \";\")\n",
    "\n",
    "# Step 1: Keep only the relevant variables in the DataFrame\n",
    "some_df = some_df[[\"ATC_Code\", \"MENGE\", \"DDD_JE_PACKUNG\"]]\n",
    "\n",
    "# Step 2: Create the ATCX column by taking the first `ATCX` characters from ATC_Code\n",
    "# Assuming `ATCX` is defined in a config file as an integer\n",
    "some_df[\"ATCX\"] = some_df[\"ATC_Code\"].str[:ATCX]\n",
    "\n",
    "# Step 3: Drop the ATC_Code column\n",
    "some_df = some_df.drop(columns=[\"ATC_Code\"])\n",
    "\n",
    "# Step 4: Keep only one row per unique ATCX value, along with the first MENGE and DDD_JE_PACKUNG\n",
    "some_df = some_df.groupby(\"ATCX\", as_index=False).first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f632e49-be55-4d73-abb8-86736e4c85a5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "PrescriptionsF = pd.merge(PrescriptionsF, some_df, on=\"ATCX\", how=\"left\")\n",
    "\n",
    "# Display the resulting DataFrame\n",
    "print(PrescriptionsF.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b387e00f-dc6b-4df1-a638-2778bb65cdcd",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Treatment part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "204d210f-4b50-4ac4-9aff-d161f1dbb0cd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "Treatments = pd.read_csv(data_path + new_version_path + \"HEMI_EVO.csv\", sep=\";\")\n",
    "Treatments = Treatments[[\"PID\", \"Leistung\",\"Leistungsdatum\"]]\n",
    "Treatments "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b06d42ab-1223-4946-9daf-0fe863c144e2",
   "metadata": {},
   "source": [
    "## Diagnostics part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "161bece7-7c05-4cb3-a751-f1825c55f543",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "Diagnostics = pd.read_csv(data_path + new_version_path + \"ARZT_DIAGNOSE.csv\", sep=\";\")\n",
    "Diagnostics = Diagnostics[[\"PID\", \"Bezugsjahr\", \"ICD_Code\", \"Behandl_Quartal\"]]\n",
    "Diagnostics[\"ICD_Code\"] = Diagnostics[\"ICD_Code\"].str[:AICDX]\n",
    "\n",
    "# Mapping quarter numbers to month abbreviations\n",
    "quarter_to_month = {1: 'JAN', 2: 'APR', 3: 'JUL', 4: 'OCT'}\n",
    "\n",
    "# Create the new date column with format '01MONYEAR'\n",
    "Diagnostics['Bezugsjahr'] = '01' + Diagnostics['Behandl_Quartal'].map(quarter_to_month) + Diagnostics['Bezugsjahr'].astype(str)\n",
    "\n",
    "# Drop the Behandl_Quartal column as requested\n",
    "Diagnostics = Diagnostics.drop(columns=['Behandl_Quartal'])\n",
    "\n",
    "Diagnostics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "267c6bc9-a405-45e7-af7a-e382752459eb",
   "metadata": {},
   "source": [
    "## Profile building"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6450f7e3-9736-4ae7-9461-19b06dcd88ff",
   "metadata": {},
   "source": [
    "### Distribution of Days between the first operation date and the the first treatment date "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d7fa587-204d-4258-aeae-2c87f2cffaa1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# DATAFRAME FUNCTION CALLING: \n",
    "profiles = PatientTimeProfiles(IStudyPopulationAS, PrescriptionsF, Treatments, Diagnostics)\n",
    "result_df = profiles.create_sparse_dataframe()\n",
    "print(result_df.head()) \n",
    "\n",
    "\n",
    "#IF RUNNING profiles.create_sparse_dataframe_first_30_days instead of create_sparse_dataframe FOR CORRELATION BETWEEN PAIN KILLER QUANTITY AND HEMI_GAP\n",
    "#RUN THIS FOLLOWING CELL, LEAVE ONE AND RUN THE NEXT, AND THEN DIRECTLY UNTIL CORRELATION CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ed02d34-e405-4243-baf3-1415c9757fba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### Distribution of Days between the first operation date and the the first treatment date \n",
    "# HYPOTHESIS: PRESCRIPTIONS AND TREATMENT CLOSE AFTER THEIR OPERATION RECOVER BETTER\n",
    "\n",
    "# Step 1: Filter Treatments to keep only PIDs that are in Study Population  reatments['PID'].isin(StudyPopulation['PID'])]\n",
    "filtered_treatments = Treatments[Treatments['PID'].isin(StudyPopulation['PID'])]\n",
    "\n",
    "\n",
    "# Step 2: Merge StudyPopulation with filtered treatments to have access to First_OP during filtering\n",
    "merged_treatments = pd.merge(filtered_treatments, StudyPopulation[['PID', 'First_OP']], on='PID', how='left')\n",
    "\n",
    "# Step 3: Filter treatments to keep only those where Leistungsdatum is after First_OP\n",
    "valid_treatments = merged_treatments[merged_treatments['Leistungsdatum'] > merged_treatments['First_OP']]\n",
    "\n",
    "# Step 4: Group by PID and get the earliest Leistungsdatum after the First_OP, calculate difference in days and replace all ones larger than observed years with 0\n",
    "earliest_treatments_after_op = valid_treatments.groupby('PID').agg({'Leistungsdatum': 'min', 'Leistung': 'first'}).reset_index()\n",
    "earliest_treatments_after_op = pd.merge(earliest_treatments_after_op, StudyPopulation[['PID', 'First_OP']], on='PID', how='left')\n",
    "earliest_treatments_after_op['days_diff'] = (earliest_treatments_after_op['Leistungsdatum'] - earliest_treatments_after_op['First_OP']).dt.days\n",
    "earliest_treatments_after_op.loc[earliest_treatments_after_op['days_diff'] > ObservationDays, ['Leistungsdatum', 'Leistung']] = np.nan\n",
    "\n",
    "#HYPOTHESIS: PEOPLE TAKING P\n",
    "# Step 5: Merge the earliest valid treatment with Study Population\n",
    "merged_df = pd.merge(StudyPopulation, earliest_treatments_after_op[[\"PID\", \"Leistungsdatum\", \"Leistung\"]], on='PID', how='left')\n",
    "\n",
    "merged_df['First_OP'] = pd.to_datetime(merged_df['First_OP'])\n",
    "merged_df['Leistungsdatum'] = pd.to_datetime(merged_df['Leistungsdatum'])\n",
    "\n",
    "# Step 6: Calculate the difference in days\n",
    "merged_df['Treatment_Gap_HEMI'] = (merged_df['Leistungsdatum'] - merged_df['First_OP']).dt.days\n",
    "\n",
    "# Step 7: Plot the distribution of the number of days\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(merged_df['Treatment_Gap_HEMI'], bins=30, edgecolor='black')\n",
    "plt.xlabel('Difference in Days')\n",
    "plt.ylabel('Number of Cases')\n",
    "plt.title('Distribution of Days Between First_OP and First Treatment')\n",
    "plt.show()\n",
    "\n",
    "# Output the final DataFrame with the calculated differences\n",
    "print(merged_df)\n",
    "\n",
    "\n",
    "#QUESTIONS: \n",
    "\n",
    "#Here, a separation for the population that received a revision and the one which would not would also be good, \n",
    "#right? Hypothesis: In the ones which did not receive revision, there shall be more cases which received treatment\n",
    "#quickly. A: YUP "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "825a3d5a-539d-47fa-bae4-3192ac7fac3e",
   "metadata": {},
   "source": [
    "### Division between Revisioned and non Revisioned populations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e8b4f63-6f9d-46cd-9a46-fbdcd905d112",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#New code: First creates an array of the same shape, 1 row for each PID with a 1 instead of the number of days, and divides each 1 over the total \n",
    "#number of PID in that subpopulation, therefore, the percentage each PID carries for the full population. Finally for each distinct value, it adds\n",
    "#the percentual contribution of each time it finds a such case.\n",
    "\n",
    "merged_df_rev = merged_df[merged_df[\"1_Revision\"] == 1]\n",
    "merged_df_no_rev = merged_df[merged_df[\"1_Revision\"] == 0]\n",
    "\n",
    "# Calculate the relative percentage for each population\n",
    "rev_weights = np.ones_like(merged_df_rev['Treatment_Gap_HEMI']) / len(merged_df_rev)\n",
    "no_rev_weights = np.ones_like(merged_df_no_rev['Treatment_Gap_HEMI']) / len(merged_df_no_rev)\n",
    "\n",
    "# Plot both histograms in the same figure\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(merged_df_rev['Treatment_Gap_HEMI'], bins=30, edgecolor='black', alpha=0.7, label='With Revision', color='blue', weights=rev_weights)\n",
    "plt.hist(merged_df_no_rev['Treatment_Gap_HEMI'], bins=30, edgecolor='black', alpha=0.7, label='Without Revision', color='orange', weights=no_rev_weights)\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('Difference in Days')\n",
    "plt.ylabel('Percentage of Cases')\n",
    "plt.title('Distribution of Days Between First_OP and First Treatment')\n",
    "plt.legend()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1048cb99-4da5-4054-b2d5-a64c8808508a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Add the Treatment_Gap_HEMI variable to the result_df Dataset\n",
    "result_df = pd.merge(result_df, merged_df[['PID', 'Treatment_Gap_HEMI']], on='PID', how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaac8784-bfb3-410a-83b5-759f14cbeb11",
   "metadata": {},
   "source": [
    "### Analysis for Prescriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a76f2b2f-15ad-4007-a336-556e7cd853df",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Step 1: Filter Prescriptions to keep only PIDs that are in Study Population\n",
    "filtered_prescriptions = PrescriptionsF[PrescriptionsF['PID'].isin(StudyPopulation['PID'])]\n",
    "\n",
    "# Step 2: Merge StudyPopulation with filtered prescriptions to have access to First_OP during filtering\n",
    "merged_prescriptions = pd.merge(filtered_prescriptions, StudyPopulation[['PID', 'First_OP']], on='PID', how='left')\n",
    "\n",
    "# Step 3: Filter prescriptions to keep only those where Verordnungsdatum is after First_OP\n",
    "valid_prescriptions = merged_prescriptions[merged_prescriptions['Verordnungsdatum'] > merged_prescriptions['First_OP']]\n",
    "\n",
    "# Step 4: Group by PID and get the earliest Verordnungsdatum after the First_OP\n",
    "earliest_prescriptions_after_op = valid_prescriptions.groupby('PID').agg({'Verordnungsdatum': 'min', 'ATCX': 'first'}).reset_index()\n",
    "earliest_prescriptions_after_op = pd.merge(earliest_prescriptions_after_op, StudyPopulation[['PID', 'First_OP']], on='PID', how='left')\n",
    "earliest_prescriptions_after_op['days_diff'] = (earliest_prescriptions_after_op['Verordnungsdatum'] - earliest_prescriptions_after_op['First_OP']).dt.days\n",
    "earliest_prescriptions_after_op.loc[earliest_prescriptions_after_op['days_diff'] > ObservationDays, ['Verordnungsdatum', 'ATCX']] = np.nan\n",
    "\n",
    "# Step 5: Merge the earliest valid treatment with Study Population\n",
    "merged_df = pd.merge(StudyPopulation, earliest_prescriptions_after_op[[\"PID\", \"Verordnungsdatum\", \"ATCX\"]], on='PID', how='left')\n",
    "\n",
    "merged_df['First_OP'] = pd.to_datetime(merged_df['First_OP'])\n",
    "merged_df['Verordnungsdatum'] = pd.to_datetime(merged_df['Verordnungsdatum'])\n",
    "\n",
    "# Step 6: Calculate the difference in days\n",
    "merged_df['Treatment_Gap_ATCX'] = (merged_df['Verordnungsdatum'] - merged_df['First_OP']).dt.days\n",
    "\n",
    "# Step 7: Plot the distribution of the number of days\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(merged_df['Treatment_Gap_ATCX'], bins=30, edgecolor='black')\n",
    "plt.xlabel('Difference in Days')\n",
    "plt.ylabel('Number of Cases')\n",
    "plt.title('Distribution of Days Between First_OP and First Treatment')\n",
    "plt.show()\n",
    "\n",
    "# Output the final DataFrame with the calculated differences\n",
    "print(merged_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86af605e-e3eb-4d7b-8c57-262418262de9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Division between population with and without Revision\n",
    "\n",
    "merged_df_rev = merged_df[merged_df[\"1_Revision\"] == 1]\n",
    "merged_df_no_rev = merged_df[merged_df[\"1_Revision\"] == 0]\n",
    " \n",
    "# Calculate the relative percentage for each population\n",
    "rev_weights = np.ones_like(merged_df_rev['Treatment_Gap_ATCX']) / len(merged_df_rev)\n",
    "no_rev_weights = np.ones_like(merged_df_no_rev['Treatment_Gap_ATCX']) / len(merged_df_no_rev)\n",
    "\n",
    "# Plot both histograms in the same figure\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(merged_df_rev['Treatment_Gap_ATCX'], bins=30, edgecolor='black', alpha=0.7, label='With Revision', color='blue', weights=rev_weights)\n",
    "plt.hist(merged_df_no_rev['Treatment_Gap_ATCX'], bins=30, edgecolor='black', alpha=0.7, label='Without Revision', color='orange', weights=no_rev_weights)\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('Difference in Days')\n",
    "plt.ylabel('Percentage of Cases')\n",
    "plt.title('Distribution of Days Between First_OP and First Prescription')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30e83788-696c-450f-a82e-529cad0bde98",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Adding of the Treatment_Gap_ATCX variable to result_df\n",
    "result_df = pd.merge(result_df, merged_df[['PID', 'Treatment_Gap_ATCX']], on='PID', how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49019f33-2856-46ea-998a-05db8f8100f4",
   "metadata": {},
   "source": [
    "## Separation of population between Pain Killers and Non Pain Killers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "177d3b42-ad87-43cd-b97b-3095282ce13c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# Step 1: Filter Prescriptions to keep only PIDs that are in Study Population\n",
    "filtered_prescriptions = PrescriptionsF[PrescriptionsF['PID'].isin(StudyPopulation['PID'])]\n",
    "\n",
    "# Step 2: Merge StudyPopulation with filtered prescriptions to have access to First_OP during filtering\n",
    "merged_prescriptions = pd.merge(filtered_prescriptions, StudyPopulation[['PID', 'First_OP']], on='PID', how='left')\n",
    "\n",
    "# Step 3: Filter prescriptions to keep only those where Verordnungsdatum is after First_OP\n",
    "valid_prescriptions = merged_prescriptions[merged_prescriptions['Verordnungsdatum'] > merged_prescriptions['First_OP']]\n",
    "\n",
    "# Step 3.5: Split the data based on ATCX code starting with 'N02' and not\n",
    "prescriptions_n02 = valid_prescriptions[valid_prescriptions['ATCX'].str.startswith('N02')]\n",
    "prescriptions_other = valid_prescriptions[~valid_prescriptions['ATCX'].str.startswith('N02')]\n",
    "\n",
    "# Step 4: Process each group separately to get the earliest Verordnungsdatum after First_OP for each PID\n",
    "# Group for prescriptions with ATCX codes starting with 'N02'\n",
    "earliest_n02_after_op = prescriptions_n02.groupby('PID').agg({'Verordnungsdatum': 'min', 'ATCX': 'first'}).reset_index()\n",
    "earliest_n02_after_op = pd.merge(earliest_n02_after_op, StudyPopulation[['PID', 'First_OP']], on='PID', how='left')\n",
    "earliest_n02_after_op['days_diff'] = (earliest_n02_after_op['Verordnungsdatum'] - earliest_n02_after_op['First_OP']).dt.days\n",
    "earliest_n02_after_op.loc[earliest_n02_after_op['days_diff'] > ObservationDays, ['Verordnungsdatum', 'ATCX']] = np.nan\n",
    "\n",
    "# Group for prescriptions with ATCX codes not starting with 'N02'\n",
    "earliest_other_after_op = prescriptions_other.groupby('PID').agg({'Verordnungsdatum': 'min', 'ATCX': 'first'}).reset_index()\n",
    "earliest_other_after_op = pd.merge(earliest_other_after_op, StudyPopulation[['PID', 'First_OP']], on='PID', how='left')\n",
    "earliest_other_after_op['days_diff'] = (earliest_other_after_op['Verordnungsdatum'] - earliest_other_after_op['First_OP']).dt.days\n",
    "earliest_other_after_op.loc[earliest_other_after_op['days_diff'] > ObservationDays, ['Verordnungsdatum', 'ATCX']] = np.nan\n",
    "\n",
    "# Step 5: Merge each group's earliest valid treatment with the Study Population\n",
    "merged_n02 = pd.merge(StudyPopulation, earliest_n02_after_op[[\"PID\", \"Verordnungsdatum\", \"ATCX\"]], on='PID', how='left')\n",
    "merged_other = pd.merge(StudyPopulation, earliest_other_after_op[[\"PID\", \"Verordnungsdatum\", \"ATCX\"]], on='PID', how='left')\n",
    "\n",
    "for df in [merged_n02, merged_other]:\n",
    "    df['First_OP'] = pd.to_datetime(df['First_OP'])\n",
    "    df['Verordnungsdatum'] = pd.to_datetime(df['Verordnungsdatum'])\n",
    "    df['Treatment_Gap_ATCX'] = (df['Verordnungsdatum'] - df['First_OP']).dt.days\n",
    "\n",
    "# Optional: Plot or analyze each group individually, for example:\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(merged_n02['Treatment_Gap_ATCX'].dropna(), bins=30, edgecolor='black', alpha=0.7, label='ATCX starts with N02')\n",
    "plt.hist(merged_other['Treatment_Gap_ATCX'].dropna(), bins=30, edgecolor='black', alpha=0.7, label='ATCX does not start with N02')\n",
    "plt.xlabel('Difference in Days')\n",
    "plt.ylabel('Number of Cases')\n",
    "plt.title('Distribution of Days Between First_OP and First Prescription')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5445d458-33d6-4543-9311-00736285f9a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Division between population with and without Revision\n",
    "\n",
    "merged_n02_rev = merged_n02[merged_n02[\"1_Revision\"] == 1]\n",
    "merged_n02_no_rev = merged_n02[merged_n02[\"1_Revision\"] == 0]\n",
    " \n",
    "# Calculate the relative percentage for each population\n",
    "rev_weights = np.ones_like(merged_n02_rev['Treatment_Gap_ATCX']) / len(merged_n02_rev)\n",
    "no_rev_weights = np.ones_like(merged_n02_no_rev['Treatment_Gap_ATCX']) / len(merged_n02_no_rev)\n",
    "\n",
    "# Plot both histograms in the same figure\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(merged_n02_rev['Treatment_Gap_ATCX'], bins=30, edgecolor='black', alpha=0.7, label='With Revision', color='blue', weights=rev_weights)\n",
    "plt.hist(merged_n02_no_rev['Treatment_Gap_ATCX'], bins=30, edgecolor='black', alpha=0.7, label='Without Revision', color='orange', weights=no_rev_weights)\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('Difference in Days')\n",
    "plt.ylabel('Percentage of Cases')\n",
    "plt.title('Distribution of Days Between First_OP and First Prescription')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98d3be78-c2db-40d0-b90d-60a2e2c8248e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Therefore, this story tells that people who got a revision, were probably under more pain in the moment of first operation, and therefore started to\n",
    "#take their pain killers before, than those who didn´t get a revision, which were also not in so much pain and in proportion didn´t start taking \n",
    "#pain killers in the same way.\n",
    "\n",
    "#HYPOTHESIS: Based on field knowledge, we believe that the larger the number of pain killers a patient takes, the more it takes them to start to \n",
    "#their therapies, and the other way around, patients without so many pain killer drugs assigned begin with their therapies before. Therefore: \n",
    "# we´ll measure the correlation between pain killer amount and number of days to first treatment. We also divide patients in groups by their painkiller\n",
    "#usage level, and see the average Treatment_Gap_HEMI to be compared betweeen groups with a t-test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4ca8d2c-33c8-426d-818a-863de2f67259",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#WARNING: RUN JUST WHEN DOING THE CORRELATION ANALYSIS BETWEEN PAIN KILLER USAGE AND HEMI GAP, RUNNING create_sparse_dataframe_first_30_days\n",
    "\n",
    "from matplotlib.ticker import PercentFormatter\n",
    "\n",
    "#STEP I\n",
    "result_df['total_painkillers'] = result_df.filter(regex='^ATCX_N02').sum(axis=1)\n",
    "\n",
    "\n",
    "\n",
    "#STEP II\n",
    "# Filtering Treatments and calculating the first treatment date after operation date\n",
    "filtered_treatments = Treatments[Treatments['PID'].isin(StudyPopulation['PID'])]\n",
    "merged_treatments = pd.merge(filtered_treatments, StudyPopulation[['PID', 'First_OP']], on='PID', how='left')\n",
    "valid_treatments = merged_treatments[merged_treatments['Leistungsdatum'] > merged_treatments['First_OP']]\n",
    "\n",
    "# Getting the earliest treatment after the operation for each patient\n",
    "earliest_treatments_after_op = valid_treatments.groupby('PID').agg({\n",
    "    'Leistungsdatum': 'min'\n",
    "}).reset_index()\n",
    "\n",
    "# Calculate days difference and handle cases beyond observed period\n",
    "earliest_treatments_after_op = pd.merge(earliest_treatments_after_op, StudyPopulation[['PID', 'First_OP']], on='PID', how='left')\n",
    "earliest_treatments_after_op['Treatment_Gap_HEMI'] = (earliest_treatments_after_op['Leistungsdatum'] - earliest_treatments_after_op['First_OP']).dt.days\n",
    "earliest_treatments_after_op.loc[earliest_treatments_after_op['Treatment_Gap_HEMI'] > ObservationDays, 'Treatment_Gap_HEMI'] = np.nan\n",
    "\n",
    "#STEP III\n",
    "# Merge the calculated treatment gap with the patient drug summary\n",
    "analysis_df = pd.merge(result_df[['PID', 'total_painkillers']], earliest_treatments_after_op[['PID', 'Treatment_Gap_HEMI']], on='PID', how='left')\n",
    "\n",
    "# Checking correlation between painkiller usage and treatment gap\n",
    "correlation = analysis_df['total_painkillers'].corr(analysis_df['Treatment_Gap_HEMI'])\n",
    "print(f\"Correlation between painkiller usage and days to first treatment: {correlation}\")\n",
    "\n",
    "\n",
    "\n",
    "#STEP IV\n",
    "# Define a threshold to categorize patients as high or low painkiller users\n",
    "painkiller_threshold = analysis_df['total_painkillers'].median()\n",
    "analysis_df['painkiller_group'] = np.where(analysis_df['total_painkillers'] >= painkiller_threshold, 'High', 'Low')\n",
    "\n",
    "# Calculate average treatment gap for each group\n",
    "group_means = analysis_df.groupby('painkiller_group')['Treatment_Gap_HEMI'].mean()\n",
    "print(group_means)\n",
    "\n",
    "# Perform t-test or Mann-Whitney U test to test for significant difference\n",
    "from scipy.stats import ttest_ind, mannwhitneyu\n",
    "\n",
    "high_group = analysis_df[analysis_df['painkiller_group'] == 'High']['Treatment_Gap_HEMI'].dropna()\n",
    "low_group = analysis_df[analysis_df['painkiller_group'] == 'Low']['Treatment_Gap_HEMI'].dropna()\n",
    "\n",
    "t_stat, p_val = ttest_ind(high_group, low_group)\n",
    "print(f\"T-test results: t-statistic={t_stat}, p-value={p_val}\")\n",
    "\n",
    "\n",
    "# STEP IV: Plotting\n",
    "\n",
    "# Plot 1: Distribution of Treatment Gap by Painkiller Usage Group\n",
    "plt.figure(figsize=(10, 6))\n",
    "for group, color in zip(['High', 'Low'], ['blue', 'orange']):\n",
    "    values = analysis_df[analysis_df['painkiller_group'] == group]['Treatment_Gap_HEMI'].dropna()\n",
    "    plt.hist(\n",
    "        values,\n",
    "        bins=30, alpha=0.7, label=f\"{group} Painkiller Usage\", color=color,\n",
    "        weights=np.ones_like(values) / len(values)\n",
    "    )\n",
    "plt.xlabel('Days to First Treatment')\n",
    "plt.ylabel('Percentage of Population')\n",
    "plt.title('Distribution of Treatment Gap by Painkiller Usage Group')\n",
    "plt.gca().yaxis.set_major_formatter(PercentFormatter(1))\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Plot 2: Boxplot of Treatment Gaps\n",
    "plt.figure(figsize=(8, 6))\n",
    "analysis_df.boxplot(column='Treatment_Gap_HEMI', by='painkiller_group', grid=False, patch_artist=True)\n",
    "plt.title('Boxplot of Treatment Gap by Painkiller Usage Group')\n",
    "plt.suptitle(\"\")  # Suppress the default title\n",
    "plt.xlabel('Painkiller Usage Group')\n",
    "plt.ylabel('Days to First Treatment')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "#Plot 3: Scatterplot Pain Killer level and Treatment Gap\n",
    "# Ensure no NaN values in the columns used for plotting\n",
    "plot_df = analysis_df[['total_painkillers', 'Treatment_Gap_HEMI']].dropna()\n",
    "plot_df_filtered = plot_df[plot_df['total_painkillers'] <= 20]\n",
    "\n",
    "# Create the scatter plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(data=plot_df_filtered, x='Treatment_Gap_HEMI', y='total_painkillers', alpha=0.7, color='blue')\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('Days to First Treatment')\n",
    "plt.ylabel('Painkiller Volume')\n",
    "plt.title('Correlation Between Painkiller Usage and Days to First Treatment')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cfe8772-c7ef-4d24-99cb-fe0e7f371272",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CORRELATION ANALYSIS\n",
    "#An unexpected low and even negative! correlation between painkiller usage and days to first treatment. In theory we expected, that when high quantities of \n",
    "#painkillers would be used, larger days to treatment would follow because of inability to join because of pain. The negative correlation sells the \n",
    "#opposite story, either way, the magnitude of the correlation is close to 0.\n",
    "\n",
    "#After splitting populations of larger than median and lower than median pain killer usage, the average for their HEMI GAPS was calculated and the t test \n",
    "#suggests there is no significative difference between the both means, also suggesting pain killer usage does not cause the number of days to \n",
    "#first treatment to be larger. \n",
    "\n",
    "#First plot even says people in pain tend to start their treatment a little bit before in comparisson to people without so much pain.\n",
    "\n",
    "#Second plot. Median for Days to First Treatment appears to be similar for both groups with high and low pain killer usage, there is also similar box \n",
    "#extension, having similar +-1 quantiles from the median. Both groups with patients with significantly larger waiting times to first treatment. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80289f7f-f0db-414e-be6d-6b4143303def",
   "metadata": {},
   "source": [
    "### Analysis for Diagnosis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0442ab6a-c49a-4ef0-98b9-f754eae45b28",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Step 1: Filter Prescriptions to keep only PIDs that are in Study Population\n",
    "filtered_diagnostics = Diagnostics[Diagnostics['PID'].isin(StudyPopulation['PID'])]\n",
    "\n",
    "# Step 2: Merge StudyPopulation with filtered prescriptions to have access to First_OP during filtering\n",
    "merged_diagnostics = pd.merge(filtered_diagnostics, StudyPopulation[['PID', 'First_OP']], on='PID', how='left')\n",
    "\n",
    "# Step 3: Filter prescriptions to keep only those where Verordnungsdatum is after First_OP\n",
    "valid_diagnostics = merged_diagnostics[merged_diagnostics['Bezugsjahr'] > merged_diagnostics['First_OP']]\n",
    "\n",
    "# Step 4: Group by PID and get the earliest Verordnungsdatum after the First_OP\n",
    "earliest_diagnostics_after_op = valid_diagnostics.groupby('PID').agg({'Bezugsjahr': 'min', 'ICD_Code': 'first'}).reset_index()\n",
    "earliest_diagnostics_after_op = pd.merge(earliest_diagnostics_after_op, StudyPopulation[['PID', 'First_OP']], on='PID', how='left')\n",
    "earliest_diagnostics_after_op['days_diff'] = (earliest_diagnostics_after_op['Bezugsjahr'] - earliest_diagnostics_after_op['First_OP']).dt.days\n",
    "earliest_diagnostics_after_op.loc[earliest_diagnostics_after_op['days_diff'] > ObservationDays, ['Bezugsjahr', 'ICD_Code']] = np.nan\n",
    "\n",
    "# Step 5: Merge the earliest valid treatment with Study Population\n",
    "merged_df = pd.merge(StudyPopulation, earliest_diagnostics_after_op[[\"PID\", \"Bezugsjahr\", \"ICD_Code\"]], on='PID', how='left')\n",
    "\n",
    "merged_df['First_OP'] = pd.to_datetime(merged_df['First_OP'])\n",
    "merged_df['Bezugsjahr'] = pd.to_datetime(merged_df['Bezugsjahr'])\n",
    "\n",
    "# Step 6: Calculate the difference in days\n",
    "merged_df['Treatment_Gap_AICDX'] = (merged_df['Bezugsjahr'] - merged_df['First_OP']).dt.days\n",
    "\n",
    "# Step 7: Plot the distribution of the number of days\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(merged_df['Treatment_Gap_AICDX'], bins=30, edgecolor='black')\n",
    "plt.xlabel('Difference in Days')\n",
    "plt.ylabel('Number of Cases')\n",
    "plt.title('Distribution of Days Between First_OP and First Diagnostics')\n",
    "plt.show()\n",
    "\n",
    "# Output the final DataFrame with the calculated differences\n",
    "print(merged_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26a41639-85b1-403b-bd22-22238300f7fc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Division between population with and without Revision\n",
    "\n",
    "merged_df_rev = merged_df[merged_df[\"1_Revision\"] == 1]\n",
    "merged_df_no_rev = merged_df[merged_df[\"1_Revision\"] == 0]\n",
    " \n",
    "# Calculate the relative percentage for each population\n",
    "rev_weights = np.ones_like(merged_df_rev['Treatment_Gap_AICDX']) / len(merged_df_rev)\n",
    "no_rev_weights = np.ones_like(merged_df_no_rev['Treatment_Gap_AICDX']) / len(merged_df_no_rev)\n",
    "\n",
    "# Plot both histograms in the same figure\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(merged_df_rev['Treatment_Gap_AICDX'], bins=30, edgecolor='black', alpha=0.7, label='With Revision', color='blue', weights=rev_weights)\n",
    "plt.hist(merged_df_no_rev['Treatment_Gap_AICDX'], bins=30, edgecolor='black', alpha=0.7, label='Without Revision', color='orange', weights=no_rev_weights)\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('Difference in Days')\n",
    "plt.ylabel('Percentage of Cases')\n",
    "plt.title('Distribution of Days Between First_OP and First Diagnostics')\n",
    "plt.legend()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cb50ccc-dbe4-4f6c-8898-597b943cb39a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Adding of the Treatment_Gap_AICDX variable\n",
    "result_df = pd.merge(result_df, merged_df[['PID', 'Treatment_Gap_AICDX']], on='PID', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8407d7f2-1a22-4000-a219-00024b970bdf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#RUN THIS EVERY TIME\n",
    "filtered_df = result_df\n",
    "#filtered_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "898d193f-424d-4619-b4ec-8096f590b095",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "filtered_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "477831a1-4dd3-41e8-9d7e-e1481fd9382a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#SKIP THIS AND WHOLE FOLLOWING SECTION IF NOT WISHING TO PRODUCE PLOTS FOR DATA SECTION\n",
    "reduced_result_df = result_df.groupby('PID', as_index=False).first()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96008d9d-e098-429f-97bd-d3deaf5261c0",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Plot producing section for Data section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74d4cba1-94e3-4484-8010-d5246fcc9b60",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Count of patients who received and did not receive revision\n",
    "revision_counts = reduced_result_df['1_Revision'].value_counts()\n",
    "\n",
    "# Plotting the counts\n",
    "plt.figure(figsize=(6, 4))\n",
    "revision_counts.plot(kind='bar', color=['blue', 'orange'], alpha=0.7)\n",
    "\n",
    "# Add counts above the bars\n",
    "for idx, value in enumerate(revision_counts):\n",
    "    plt.text(x=idx, y=value + 0.5, s=str(value), ha='center', fontsize=10)\n",
    "    \n",
    "plt.title(\"Count of Patients: Revision vs. No Revision\")\n",
    "plt.xlabel(\"Revision Status (0 = No, 1 = Yes)\")\n",
    "plt.ylabel(\"Count of Patients\")\n",
    "plt.xticks(rotation=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4713d30-a3a2-4f58-9823-6b30ce454fbf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Calculate statistics\n",
    "age_stats = reduced_result_df['Age'].describe()\n",
    "print(f\"Age Statistics:\\n{age_stats}\")\n",
    "\n",
    "\n",
    "# Boxplot\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.boxplot(reduced_result_df['Age'], vert=False, patch_artist=True, boxprops=dict(facecolor='skyblue', color='black'))\n",
    "plt.title(\"Age Boxplot\")\n",
    "plt.xlabel(\"Age\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c79f0034-b298-4ec4-ac80-ea9d30cc1a57",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Count of 1s and 2s\n",
    "sex_count = reduced_result_df['Sex'].value_counts()\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(6, 4))\n",
    "sex_count.plot(kind='bar', color=['green', 'red'], alpha=0.7)\n",
    "plt.title(\"Count of 1s and 2s in Sex\")\n",
    "plt.xlabel(\"Sex Group\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.xticks(rotation=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5f97d48-6a52-4ddc-a198-1ca7f45fbb47",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# List of treatment gaps\n",
    "treatment_gaps = ['Treatment_Gap_ATCX', 'Treatment_Gap_HEMI', 'Treatment_Gap_AICDX']\n",
    "\n",
    "for gap in treatment_gaps:\n",
    "    # Restore original values if previously transformed\n",
    "    reduced_result_df[gap] = np.exp(reduced_result_df[gap])  # Uncomment if needed\n",
    "\n",
    "    print(f\"\\nStatistics for {gap} (Restored Values):\\n{reduced_result_df[gap].describe()}\")\n",
    "   \n",
    "    # Quantile plot\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    reduced_result_df[gap].quantile([0.0, 0.25, 0.5, 0.75, 1.0]).plot(kind='bar', color='purple')\n",
    "    plt.title(f\"Quantiles for {gap}\")\n",
    "    plt.xlabel(\"Quantiles\")\n",
    "    plt.ylabel(\"Days\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "251ca144-19fe-4315-8e20-7499530a0f37",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "columns_of_interest = ['ATCX_N02A', 'HEMI_0', 'AICDX_F32']\n",
    "\n",
    "# Count rows with a 1 for each column\n",
    "subperiod_counts = {}\n",
    "total_rows = len(result_df)\n",
    "\n",
    "for column in columns_of_interest:\n",
    "    count = result_df[column].sum()\n",
    "    proportion = count / total_rows\n",
    "    subperiod_counts[column] = {\"Count\": count, \"Proportion\": proportion}\n",
    "    print(f\"{column}: {count} rows with 1, {proportion:.2%} of subperiods\")\n",
    "\n",
    "# Convert to DataFrame for visualization\n",
    "subperiod_df = pd.DataFrame.from_dict(subperiod_counts, orient='index')\n",
    "\n",
    "# Plot proportions\n",
    "subperiod_df['Proportion'].plot(kind='bar', color='teal', alpha=0.7, figsize=(6, 4))\n",
    "plt.title(\"Proportion of Subperiods with Assigned Variables\")\n",
    "plt.xlabel(\"Variable\")\n",
    "plt.ylabel(\"Proportion\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a6804ef-886c-4764-8d45-57b67976e816",
   "metadata": {},
   "source": [
    "### Modeling little research"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9afc0273-37a1-4c10-886c-a8f50255921c",
   "metadata": {},
   "source": [
    "##### Modeling options: Markov Models: Assumptions: Whether a patient will receive a revision or not depends on the current moment, and not in the sequence of of moments before. The health status of the patient is hidden and only seen through the medication and treatment he receives. Here, hidden states would be health states of the patient. Next state depends only on current one. There is a probability distribution for the assignment of prescriptions and treatments for each hidden state. Finally, whether the patient receives a revision or not is determined by the path of states of the patient. With probabilities to transition from one state to another. Could use the EM algorithm to train the model. You could use your model to predict states of patients given a path, or the probability of needing revision. May create over simple models and the assumptions might not be fulfilled. \n",
    "\n",
    "##### LSTM: Capture sequential patterns. There is no such assumption that the probability of needing revision is just influenced by the immediate state before, but are built to capture long term relationships in sequences. The model would predict the probability of needed revision at each time step of the path. Frequencies of prescriptions or dosages and patient characteristics could also be features of our model. Train using binary cross entropy loss. Non linear relationships can be modeled by LSTM. A lot of data is needed to train them. Black boxy. \n",
    "\n",
    "##### Posibility: Transformers: They process the entire sequence at a single step. Use attention to compare each element of the sequence to the other elements. They can speed up. They are more transparent than LSTMs. LSTMs need less data, for small datasets, they can overfit easily. Apparently because of the size of the data we are working with, LSTM seems better.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "865de656-9ea1-4255-be1f-b0e4df02ab52",
   "metadata": {},
   "source": [
    "## Preliminary Decision Tree Model, Evaluation metrics and Shapley Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d480e47-26b7-463a-97bd-f3bbdcaea212",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Creation of Data for Decision Tree feeding: One line per PID covering the full time '\n",
    "#PID: Tomar uno, todo igual. Quita subperiod, PeriodStart y PeriodEnd, 1_Revision y Sex el que sea, Age es del \n",
    "#primer subperiodo, ATCX HEMI AICDX: Si es 1 en algún subperiodo es 1, else 0. Gaps el que sea.\n",
    "\n",
    "grouped = filtered_df.groupby('PID')\n",
    "\n",
    "# Aggregate the data\n",
    "filtered_df_DT = grouped.agg(\n",
    "    {\n",
    "        '1_Revision': 'first',\n",
    "        'Sex': 'first',\n",
    "        'Age': 'first',\n",
    "        'Treatment_Gap_ATCX': 'first',\n",
    "        'Treatment_Gap_HEMI': 'first',\n",
    "        'Treatment_Gap_AICDX': 'first',\n",
    "        # Apply logical OR to ATCX_*, HEMI_*, and AICDX_* columns\n",
    "        **{col: 'max' for col in filtered_df.columns if col.startswith(('ATCX_', 'HEMI_', 'AICDX_'))},\n",
    "    }\n",
    ").reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76a55788-69e0-4d6a-8159-48d6f6ebb2d9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#RUN SAME\n",
    "\n",
    "# Step 1: Filter features based on threshold\n",
    "threshold = shap_threshold  # Adjust the threshold value as needed\n",
    "\n",
    "if shap_threshold_level == \"very_low\": \n",
    "    threshold = threshold * 0.00000000000000000001\n",
    "elif shap_threshold_level == \"low\":\n",
    "    threshold = threshold * 0.001\n",
    "elif shap_threshold_level == \"medium\":\n",
    "    threshold = threshold * 0.01\n",
    "elif shap_threshold_level == \"high\": \n",
    "    threshold = threshold * 0.1\n",
    "    \n",
    "important_features = original_feature_importance[original_feature_importance['Importance'] >= threshold]['Feature'].tolist()\n",
    "\n",
    "# Step 2: Add any essential columns that should always be kept (e.g., PID, Subperiod)\n",
    "essential_columns = ['PID', '1_Revision', 'Sex', 'Age']\n",
    "columns_to_keep = essential_columns + important_features\n",
    "\n",
    "# Step 3: Filter `result_df` to keep only the specified columns\n",
    "filtered_df_DT = filtered_df_DT[columns_to_keep]\n",
    "filtered_df_DT = filtered_df_DT.loc[:, ~filtered_df_DT.columns.duplicated()]\n",
    "#filtered_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e81cd04e-065c-4130-9177-d78cfe94c843",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#DISCONTINUED SINCE WE ENDED UP RUNNING JUST WITH HIGH THRESHOLD\n",
    "#CODE FOR THRESHOLD UPDATE (FOR THE TIME FRAME/RESOLUTION COMBINATION) AND LEVEL OF SEVERITY \n",
    "#RUN AFTER SHAP THRESHOLD OF SHAP THRESHOLD LEVEL HAVE BEEN UPDATED\n",
    "importlib.reload(Config_V3)\n",
    "shap_threshold = Config_V3.shap_threshold\n",
    "shap_threshold_level = Config_V3.shap_threshold_level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8902aa3e-9d8b-493c-95db-ac4a9d52f89a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#I AM GETTING ONLY 80 FEATURES HAVE AN IMPORTANCE VALUE HIGER THAN 0 FROM ALL 2200. HIGHER THAN A .1 PER CENT LIKE LEANDRA\n",
    "# ONLY 22 ATTRIBUTES, HIGHER THAN A 1 PER CENT ONLY 6 VARIABLES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a677a37d-ca42-424a-9b2e-331f52d6342d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Step 1: Filter for Subperiod == 1 to only use 1 entry data per PID\n",
    "#filtered_df1 = filtered_df[filtered_df['Subperiod'] == 1]\n",
    "filtered_df1 = filtered_df_DT\n",
    "\n",
    "# Step 2: Drop unnecessary columns\n",
    "filtered_df1 = filtered_df1.drop(columns=['PID'])\n",
    "\n",
    "# Step 3: Prepare data for classification, division between predictors and target and between training and testing set\n",
    "X = filtered_df1.drop(columns=['1_Revision'])\n",
    "y = filtered_df1['1_Revision']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n",
    "\n",
    "# Step 4: Fit a classification tree model\n",
    "model = DecisionTreeClassifier(random_state=0)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Step 5: Make predictions on the test set\n",
    "y_pred_proba = model.predict_proba(X_test)[:, 1]  # Probability estimates for the positive class\n",
    "y_pred = model.predict(X_test)  # Binary predictions\n",
    "\n",
    "# Step 6: Calculate ROC curve and AUC. fpr False Positive Rate, or, rate at which data points which should have been classified as negative, were as \n",
    "#positive, (false positives between all data points which should have been negative) tpr, same but for positives. AUC explanation below, as well as \n",
    "#threshold explanation\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_pred_proba)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "print(f\"AUC: {roc_auc}\")\n",
    "\n",
    "# Plot ROC Curve. Explanation below\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--') #This is the diagonal line which represents random guessing, the worst possible performance\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n",
    "\n",
    "# Step 7: Confusion Matrix\n",
    "# SubStep 1: Create the confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# SubStep 2: Create a DataFrame for better labeling\n",
    "cm_df = pd.DataFrame(cm, index=['Actual Negative', 'Actual Positive'], \n",
    "                     columns=['Predicted Negative', 'Predicted Positive'])\n",
    "\n",
    "# SubStep 3: Print the confusion matrix with labels\n",
    "print(\"Confusion Matrix with labels:\")\n",
    "print(cm_df)\n",
    "\n",
    "# SubStep 4: (Optional) Plot the confusion matrix for better visualization\n",
    "plt.figure(figsize=(6,4))\n",
    "sns.heatmap(cm_df, annot=True, fmt='g', cmap='Blues')\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.ylabel('Actual')\n",
    "plt.xlabel('Predicted')\n",
    "plt.show()\n",
    "\n",
    "# Step 8: Youden's Index Calculation. Explanation for sensitivity, specificity and Youden Index down\n",
    "sensitivity = tpr\n",
    "specificity = 1 - fpr\n",
    "youden_index = sensitivity + specificity - 1\n",
    "optimal_idx = np.argmax(youden_index)\n",
    "optimal_threshold = thresholds[optimal_idx] #Threshold value minimizing the Youden´s Index\n",
    "optimal_youden = youden_index[optimal_idx]\n",
    "\n",
    "print(f\"Optimal Youden's Index: {optimal_youden}\")\n",
    "print(f\"Optimal Threshold: {optimal_threshold}\")\n",
    "\n",
    "#Each dot represents an observation, blue dots are lower values of the feature and red higher ones. \n",
    "#A positive SHAP value means the feature pushes the prediction towards a higher output. \n",
    "#In the case of binary variables, points are red if they received the value of 1, and blue if they received a 0. SHAP values indicate for the particular \n",
    "#case where the medication or treatment was or was not assigned contribute to increasing or decreasing the prediction\n",
    "\n",
    "# Step 10: SHAP Analysis\n",
    "# Initialize the SHAP explainer\n",
    "explainer = shap.TreeExplainer(model)\n",
    "\n",
    "# Calculate SHAP values for each datapoint (How much features impact each prediction)\n",
    "shap_values = explainer.shap_values(X_test)\n",
    "\n",
    "#Shape of the shap_values array with the SHAP values for each feature. Then also the shape of the test set\n",
    "print(f\"SHAP values shape: {shap_values.shape}\")\n",
    "print(f\"X_test shape: {X_test.shape}\")\n",
    "\n",
    "# Since we're doing binary classification, we usually focus on the positive class. It could be we are having a list containing SHAP values for the \n",
    "#0 class in the first place and class 1 in the second\n",
    "    \n",
    "if isinstance(shap_values, list):\n",
    "    shap_values_class_1 = shap_values[1][:, :, 1]  # Focus only on the second class\n",
    "else:\n",
    "    shap_values_class_1 = shap_values[:, :, 1]  # If it's not a list, directly slice the second class\n",
    "    \n",
    "#The shape of the postive class array\n",
    "print(f\"SHAP values for class 1 shape: {shap_values_class_1.shape}\")\n",
    "\n",
    "#Average SHAP values for each feature across samples \n",
    "importance_values = np.abs(shap_values_class_1).mean(axis=0)  # Mean across the first dimension (samples)\n",
    "\n",
    "#Check number of dimensions in the importance_values array. Each feature should have just 1 importance value. Take the mean if there are many, \n",
    "#averages importance values for a feature across samples\n",
    "if importance_values.ndim > 1:\n",
    "    importance_values = np.mean(importance_values, axis=1)\n",
    "\n",
    "    #Debug knowing number of features and shape of the final importance array, they shall be same sized\n",
    "print(f\"Length of X.columns: {len(X.columns)}\")\n",
    "print(f\"Shape of importance_values after mean calculation: {importance_values.shape}\")\n",
    "\n",
    "\n",
    "#Verifies the condition was actually satisfied and creates DataFrame feature/importance value\n",
    "# Create a DataFrame for feature importance\n",
    "if len(X.columns) == importance_values.shape[0]:\n",
    "    feature_importance = pd.DataFrame({\n",
    "        'Feature': X.columns,\n",
    "        'Importance': importance_values\n",
    "    })\n",
    "\n",
    "# Sort feature importance\n",
    "    feature_importance = feature_importance.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "    #Print the sorted feature importance based on the SHAP values. Or prints error if there is a mismatch between number of features and importance values\n",
    "    pd.options.display.max_rows = None\n",
    "# Display feature importance\n",
    "    print(\"Feature Importance based on SHAP values:\")\n",
    "    print(feature_importance.head(1000))\n",
    "else:\n",
    "    print(\"Shape mismatch between features and importance values!\")\n",
    "\n",
    "# Plot SHAP summary plot for class 1\n",
    "shap.summary_plot(shap_values_class_1, X_test)  # Plotting the first class SHAP values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddd6ef6c-bd74-457d-82b7-bf362eca759a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#RUN THIS, ONLY AFTER THE FIRST RUN WITHOUT ANY FEATURE SELECTION\n",
    "original_feature_importance = feature_importance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e71bbde0-330c-45ca-b880-6935d9f58ebd",
   "metadata": {},
   "source": [
    "## Bayesian Parameter Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fbdc0a4-960c-4613-a8eb-01fb5f3a3c70",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define parameter space for Bayesian search\n",
    "param_space = {\n",
    "    'criterion': ['gini', 'entropy'],\n",
    "    'max_depth': (Bay_max_depth_low, Bay_max_depth_high),\n",
    "    'min_samples_split': (Bay_min_samples_split_low, Bay_min_samples_split_high),\n",
    "    'min_samples_leaf': (Bay_min_samples_leaf_low, Bay_min_samples_leaf_high),\n",
    "    'max_features': ['sqrt', 'log2', None]\n",
    "}\n",
    "\n",
    "# Initialize Bayesian Search\n",
    "bayes_search = BayesSearchCV(estimator=DecisionTreeClassifier(random_state=0),\n",
    "                             search_spaces=param_space,\n",
    "                             scoring='roc_auc',\n",
    "                             n_iter=Bay_n_iter,  # Number of iterations to run\n",
    "                             cv=5,\n",
    "                             verbose=1)\n",
    "\n",
    "# Fit Bayesian search\n",
    "bayes_search.fit(X_train, y_train)\n",
    "\n",
    "# Best hyperparameters\n",
    "print(f\"Best parameters found: {bayes_search.best_params_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb976d2a-31fe-4a2a-a1f2-60b57db49ddd",
   "metadata": {},
   "source": [
    "## Model Re fit making use of found parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d60f9d4-4110-439a-9005-cefb6e50cadb",
   "metadata": {},
   "source": [
    "###### ROC Curve: Evaluate the performance of a binary classification model. Plots the TPR (True Positive Rate or Recall). True Positives over True Positives + False Negatives, so, from all cases which should have been classified as Positive, how many of them where? and FPR (False Positive Rate)False Positives over False Positives + True Negatives, from all the ones which should have been classified as negative, how many where classifier as positive? The y axis is the TPR and the x axis is the FPR. Each point represents the trade off at different classification thresholds (Starting this value for the features the case will be classified as positive). A good curve will head to touch the upper left corner. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be81402d-68ac-4d28-90df-8dadf327cc3b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Best parameters from Bayesian Search \n",
    "best_params = bayes_search.best_params_\n",
    "\n",
    "# Step 1: Initialize the model with the best parameters\n",
    "model = DecisionTreeClassifier(\n",
    "    criterion=best_params['criterion'],\n",
    "    max_depth=best_params['max_depth'],\n",
    "    max_features=best_params['max_features'],\n",
    "    min_samples_leaf=best_params['min_samples_leaf'],\n",
    "    min_samples_split=best_params['min_samples_split'],\n",
    "    random_state=0  # Keep random_state for reproducibility\n",
    ")\n",
    "\n",
    "# Step 2: Refit the model on the training data\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Step 3: Make predictions and evaluate the model\n",
    "y_pred = model.predict(X_test)\n",
    "y_proba = model.predict_proba(X_test)[:, 1]  # For AUC calculation\n",
    "\n",
    "# Step 4: Calculate the AUC score\n",
    "auc_score = roc_auc_score(y_test, y_proba)\n",
    "\n",
    "print(f\"Updated model AUC score: {auc_score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f0ec4ac-d734-4b98-8fef-a85c36f2d996",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(filtered_df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b9ff455-a49a-468d-8872-01f928a747bc",
   "metadata": {},
   "source": [
    "###### AUC: Area under the ROC Curve, single scalar summarizing the performance of the model. 1 is perfect classifier and 0 is Worse than random guessing. Confusion Matrix: On x axis: Actual and on y axis: Predicted and so you have True Positive, False Negative, False Positive and True Negative. Finally, the Youden  Index takes into account the TPR and the TNR (From all the ones that should have been negative, how many were actually?). J = TPR + TNR -1. This metric is from 0 to 1, (if everything is perfectly classified, both have the value of 1, less 1 is 1. It can also be 0, here it is assumed that the worst possible result is random guessing, because actually, if you had a model which perfectly clasifies everything incorrectly, you could simple switch the value of the classification and it would result in a perfectmodel. Therefore, the worst possible value is 0, which occurs if both metrics get .5 for a -1 to total 0.) It provides the best tradeoff between True Positives and False Positives. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7913fe9f-63c3-48f0-a67c-bda7f580c101",
   "metadata": {},
   "source": [
    "## LSTM Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8240e93-ef63-4f54-9417-d03d5e7d12a4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Data expected in 3D format (samples, timesteps, features)\n",
    "\n",
    "# Step 1: Sort by PID and Subperiod\n",
    "#filtered_result_df2 = result_df.sort_values(by=['PID', 'Subperiod'])\n",
    "filtered_df = filtered_df.loc[:, ~filtered_df.columns.duplicated()].copy()\n",
    "filtered_df = filtered_df.sort_values(by=['PID', 'Subperiod'])\n",
    "filtered_df = filtered_df[columns_to_keep]\n",
    "\n",
    "# Step 2: Features to include for LSTM (excluding PID, Subperiod, PeriodStart, PeriodEnd)\n",
    "features = []\n",
    "optional_features = ['Age', 'Sex', 'Treatment_Gap_HEMI', 'Treatment_Gap_ATCX', 'Treatment_Gap_AICDX']  # Include ATCX, HEMI columns too\n",
    "for col in optional_features:\n",
    "    if col in filtered_df.columns:\n",
    "        features.append(col)\n",
    "        \n",
    "atcx_cols = [col for col in filtered_df.columns if col.startswith('ATCX')]\n",
    "hemi_cols = [col for col in filtered_df.columns if col.startswith('HEMI')]\n",
    "icd_cols = [col for col in filtered_df.columns if col.startswith('AICDX')]\n",
    "features = ['Age', 'Sex', 'Treatment_Gap_HEMI', 'Treatment_Gap_ATCX', 'Treatment_Gap_AICDX']\n",
    "features = [col for col in features if col in filtered_df.columns] + atcx_cols + hemi_cols + icd_cols\n",
    "\n",
    "if 'Age' in filtered_df.columns:\n",
    "    filtered_df['Age'].fillna(filtered_df['Age'].mean(), inplace=True)\n",
    "if 'Sex' in filtered_df.columns:\n",
    "    sex_mode = filtered_df['Sex'].mode()\n",
    "    if not sex_mode.empty:  # Check if there is a mode\n",
    "        filtered_df['Sex'].fillna(sex_mode.iloc[0], inplace=True)  # Use .iloc[0] instead of [0]\n",
    "    else:\n",
    "        # Handle the case when there's no mode (optional, depends on your case)\n",
    "        filtered_df['Sex'].fillna(filtered_df['Sex'].mode().mode[0], inplace=True)\n",
    "        \n",
    "        #filtered_df['Sex'].fillna(sex_mode[0], inplace=True)\n",
    "#filtered_result_df2.fillna(value={'Age': filtered_result_df2['Age'].mean(), 'Sex': filtered_result_df2['Sex'].mode()[0]}, inplace=True)\n",
    "\n",
    "\n",
    "# Step 3: Convert the dataframe into a 3D array (samples, timesteps, features)\n",
    "X = []\n",
    "y = []\n",
    "\n",
    "if TimeResolution == \"Quarters\":  \n",
    "    num_subperiods = ObservationDays/90\n",
    "    \n",
    "if TimeResolution == \"Months\":\n",
    "    num_subperiods = ObservationDays/30\n",
    "    \n",
    "    \n",
    "\n",
    "# Step 4: Group by PID and convert to sequences\n",
    "for pid, group in filtered_df.groupby('PID'):\n",
    "    \n",
    "    #group = group[group.columns.intersection(filtered_df.columns)]\n",
    "    \n",
    "    if len(group) == num_subperiods:  # Ensure that all subperiods are present\n",
    "        X.append(group[features].values)  # Get the features as 2D array for each patient\n",
    "        y.append(group['1_Revision'].iloc[0])  # Revision surgery target (same for each subperiod)\n",
    "\n",
    "X = np.nan_to_num(np.array(X))  # Shape: (num_patients, num_subperiods, num_features)\n",
    "y = np.array(y)  # Shape: (num_patients,)\n",
    "\n",
    "X[np.isnan(X)] = 0\n",
    "\n",
    "# Step 5: Split into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "print(\"Shape of X_train:\", X_train.shape)  # Should be (num_patients, num_subperiods, num_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a252ab5-089c-4732-ba91-fed62083b9fe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Step 1: Fill NaNs i                                                                                                                                               n X with zeros before scaling\n",
    "# Step 2: Flatten and scale the dataset, treating constant columns separately\n",
    "# Reshape to (num_samples * num_timesteps, num_features) to scale the entire dataset\n",
    "num_features = X_train.shape[2] #Features is in the third position and the count starts from 0\n",
    "X_train_flat = X_train.reshape(-1, num_features) # Makes instead of separate number of patients and number of periods for each, a column like the origin\n",
    "X_test_flat = X_test.reshape(-1, num_features)\n",
    "\n",
    "# Find constant columns in X_train and X_test\n",
    "constant_columns = [col for col in range(num_features) if np.ptp(X_train_flat[:, col]) == 0]\n",
    "print(\"Constant columns:\", constant_columns)\n",
    "\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_flat_scaled = X_train_flat.copy()\n",
    "X_test_flat_scaled = X_test_flat.copy()\n",
    "\n",
    "# Scale only columns that aren't constant\n",
    "non_constant_columns = [col for col in range(num_features) if col not in constant_columns]\n",
    "X_train_flat_scaled[:, non_constant_columns] = scaler.fit_transform(X_train_flat[:, non_constant_columns])\n",
    "X_test_flat_scaled[:, non_constant_columns] = scaler.transform(X_test_flat[:, non_constant_columns])\n",
    "\n",
    "# Reshape back to the original 3D shape\n",
    "X_train_scaled = X_train_flat_scaled.reshape(X_train.shape)\n",
    "X_test_scaled = X_test_flat_scaled.reshape(X_test.shape)\n",
    "\n",
    "# Verify no NaNs in X_train_scaled or X_test_scaled\n",
    "assert not np.isnan(X_train_scaled).any(), \"NaNs found in X_train_scaled\"\n",
    "assert not np.isnan(X_test_scaled).any(), \"NaNs found in X_test_scaled\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad4ac6a1-dd93-4a47-80fe-3aeebcef2fef",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Step 1: Creates the model and establishes ranges of values for distint parameters like: number of units, adds dropout (unactivataing some neurons for\n",
    "#overfitting avoiding purposes), adds the output layer with sigmoid activation, sets the range for the learning rate, compiles the model with \n",
    "#ADAM (Combination of accumulation of gradient for speeding up the process), names the loss as the Binary Cross Entropy\n",
    "    \n",
    "def build_model(hp):\n",
    "    model = Sequential()\n",
    "    \n",
    "    # Tuning the number of LSTM units\n",
    "    model.add(Conv1D(\n",
    "        filters=hp.Int('filters', min_value=units_min_value, max_value=units_max_value, step=units_step),\n",
    "        kernel_size=hp.Int('kernel_size', min_value=2, max_value=5),\n",
    "        activation='relu',\n",
    "        padding = 'same',\n",
    "        input_shape=(X_train.shape[1], X_train.shape[2])\n",
    "    ))\n",
    "    \n",
    "    \n",
    "    model.add(Bidirectional(LSTM(\n",
    "        units=hp.Int('units_0', min_value=units_min_value, max_value=units_max_value, step=units_step),\n",
    "        return_sequences=True  # Add this line\n",
    "    )))\n",
    "\n",
    "    # Second LSTM layer with return_sequences=True to pass 3D data onward\n",
    "    model.add(LSTM(\n",
    "        units=hp.Int('units_1', min_value=units_min_value, max_value=units_max_value, step=units_step),\n",
    "        return_sequences=True,\n",
    "    ))\n",
    "    \n",
    "    # Third LSTM layer with return_sequences=False to output 2D data for Dense layer\n",
    "    model.add(LSTM(\n",
    "        units=hp.Int('units_2', min_value=units_min_value, max_value=units_max_value, step=units_step),\n",
    "        return_sequences=False,\n",
    "    ))\n",
    "\n",
    "    # Tuning the dropout rate\n",
    "    model.add(Dropout(rate=hp.Float('dropout', min_value=dropout_min_value, max_value=dropout_max_value, step=dropout_step)))\n",
    "    \n",
    "    # Output layer\n",
    "    model.add(Dense(1, activation='sigmoid', kernel_regularizer=l2(0.01)))\n",
    "    \n",
    "    # Tuning the learning rate\n",
    "    learning_rate = hp.Float('learning_rate', min_value=learning_rate_min_value, max_value=learning_rate_max_value, sampling='log')\n",
    "    model.compile(optimizer=Adam(learning_rate=learning_rate),\n",
    "                  loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30b9ad17-bee8-4a6d-84d8-3e7889396c68",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Step 2: Sets the Bayesian Optimization procedure up: Sets the objective as increasing the validation accuracy, the max number of trials as 20\n",
    "#HERE THERE IS A PARAMETER WHICH CAN BE EXTERNALIZED\n",
    "\n",
    "def initialize_tuner():\n",
    "    \"\"\"Initializes the Bayesian Optimization tuner.\"\"\"\n",
    "    tuner = kt.BayesianOptimization(\n",
    "        build_model,\n",
    "        objective='val_accuracy',  # Or use 'val_loss' for minimizing validation loss\n",
    "       max_trials=tuner_max_trials,\n",
    "        directory='lstm_tuning',\n",
    "        project_name='lstm_hyperparameter_tuning'\n",
    "    )\n",
    "    tuner.search_space_summary()\n",
    "    return tuner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0bfff35-c22e-4cc3-9c95-248945d6fcfa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Step 3: Passes training and testing data, sets the size of the batches (subparts of the data which will be used for training and testing), stocks the \n",
    "#hyperparmeter values resulting in the best performance value\n",
    "\n",
    "#HERE THERE IS ANOTHER PARAMETER WHICH CAN BE EXTERNALIZED\n",
    "def perform_hyperparameter_search(tuner, X_train, y_train, X_test, y_test):\n",
    "    \"\"\"Performs hyperparameter tuning on the LSTM model.\"\"\"\n",
    "    tuner.search(X_train, y_train, epochs=20, batch_size=16, validation_data=(X_test, y_test))\n",
    "    best_hps = tuner.get_best_hyperparameters(num_trials=5)[0]\n",
    "    print(f\"\"\"\n",
    "    The optimal number of units in the LSTM layer is {best_hps.get('units')}.\n",
    "    The optimal dropout rate is {best_hps.get('dropout')}.\n",
    "    The optimal learning rate is {best_hps.get('learning_rate')}.\n",
    "    \"\"\")\n",
    "    return best_hps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de35a1d4-f2bd-4a36-bdc3-bcec7225c2f5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Step 4: Fits model with the best hyperparameters, calculates the accuracy and then calculates probabilities for the testing subset for revision\n",
    "#Finally prints the AUC\n",
    "\n",
    "#HERE THERE ARE MORE PARAMETERS WHICH CAN BE EXTERNALIZED\n",
    "\n",
    "def train_and_evaluate(best_hps, X_train, y_train, X_test, y_test):\n",
    "    \"\"\"Builds, trains, and evaluates the model using the best hyperparameters.\"\"\"\n",
    "    best_model = build_model(best_hps)\n",
    "    history = best_model.fit(X_train, y_train, epochs=num_epochs, batch_size= training_batch_size, validation_data=(X_test, y_test))\n",
    "    loss, accuracy = best_model.evaluate(X_test, y_test)\n",
    "    print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "    # Generate predicted probabilities\n",
    "    y_pred_prob = best_model.predict(X_test).flatten()\n",
    "    y_pred = (y_pred_prob >= 0.5).astype(int) \n",
    "    \n",
    "    # Calculate confusion matrix\n",
    "    tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "    print(f\"Confusion Matrix:\\nTP: {tp}, FP: {fp}, TN: {tn}, FN: {fn}\")\n",
    "\n",
    "    # Calculate AUC\n",
    "    auc = roc_auc_score(y_test, y_pred_prob)\n",
    "    print(f\"Test AUC: {auc:.4f}\")\n",
    "\n",
    "    return best_model, history  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84d21103-6cee-4b9f-97b2-089c8b55b186",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Step 5: Accuracy and loss plots\n",
    "\n",
    "def plot_accuracy(history):\n",
    "    \"\"\"Plots training and validation accuracy over epochs.\"\"\"\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
    "    plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "    plt.title('Model Accuracy')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.show()\n",
    "\n",
    "def plot_loss(history):\n",
    "    \"\"\"Plots training and validation loss over epochs.\"\"\"\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(history.history['loss'], label='Train Loss')\n",
    "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "    plt.title('Model Loss')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f1a89c1-dcd7-4619-aff7-13883db9ec00",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Calls function by function: Calls the tuner, searches hyperparameters, trains and evaluates the best model found, plots accuracy and loss\n",
    "\n",
    "def main(num_runs=8, seed_value=42):\n",
    "    \n",
    "    set_seed(seed_value)\n",
    "    \n",
    "    best_models = []\n",
    "    best_histories = []\n",
    "    \n",
    "    \n",
    "    for i in range(num_runs):\n",
    "        print(f\"\\nRun {i + 1} of {num_runs}\")\n",
    "    # Initialize the tuner\n",
    "        tuner = initialize_tuner()\n",
    "\n",
    "    # Perform hyperparameter search\n",
    "        best_hps = perform_hyperparameter_search(tuner, X_train_scaled, y_train, X_test_scaled, y_test)\n",
    "\n",
    "    # Train and evaluate the model with best hyperparameters\n",
    "        best_model, history = train_and_evaluate(best_hps, X_train_scaled, y_train, X_test_scaled, y_test)\n",
    "\n",
    "    # Plot accuracy and loss\n",
    "        best_models.append(best_model)\n",
    "        best_histories.append(history)\n",
    "        \n",
    "        print(f\"End of run {i + 1}\")\n",
    "        \n",
    "    plot_accuracy(history)\n",
    "    plot_loss(history)\n",
    "\n",
    "# Run the main function if this file is executed\n",
    "if __name__ == \"__main__\":\n",
    "    main(num_runs=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "578c12f4-1090-4009-a385-37f6d16962a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We are seeing overfitting beahaviour: with training accuracy going up very high, training loss going very low, but testing accuracy actually going down\n",
    "#and testing loss actually going up as epochs evolve"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81b90633-7b1f-4266-9272-b2a74373134c",
   "metadata": {},
   "source": [
    "###### Difference between Accuracy and AUC: \n",
    "\n",
    "###### Accuracy is just total number of correct predictions over Total number of predictions (In a data set with \n",
    "###### example 995 positives and only 5 negatives, if you were predicting all positives the model would be \n",
    "###### 95 per cent right, but totally uncapable of identifying the negative cases)\n",
    "\n",
    "###### AUC splits to look at how many of the ones which should have been positive, actually were, and how many \n",
    "###### of the ones which should have been negative, actually were. Obviously, with our kind of problem, AUC is \n",
    "###### preferred. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aa032a9-c704-47f2-bb52-70d6ec02dda6",
   "metadata": {},
   "source": [
    "### LSTM Reimplementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cf6f34c-8b68-4d93-a421-e831bbe18c68",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#New libraries\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "567678b6-9c2f-49ca-95c9-505d452e7b1a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "match TimeResolution:\n",
    "        case \"Months\": \n",
    "            increment_days =  29\n",
    "        case \"Quarters\": \n",
    "            increment_days = 89"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f84bf45-d29e-49d5-8221-0b10700307b0",
   "metadata": {},
   "source": [
    "#### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64cf0362-8916-4c18-add1-61d4fd44589a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Preprocessing function\n",
    "def preprocess_data(df, time_span_days=ObservationDays, time_resolution_days= increment_days + 1):\n",
    "    # Calculate the number of subperiods for each patient\n",
    "    df['SubperiodLength'] = (df['PeriodEnd'] - df['PeriodStart']).dt.days\n",
    "\n",
    "    # Select relevant columns\n",
    "    numeric_features = ['Age', 'Treatment_Gap_ATCX', 'Treatment_Gap_HEMI', 'Treatment_Gap_AICDX', 'Sex']\n",
    "    binary_features = [col for col in df.columns if col.startswith(('ATCX', 'HEMI', 'AICDX'))]\n",
    "    target_column = '1_Revision'\n",
    "\n",
    "    # Impute missing values\n",
    "    numeric_imputer = SimpleImputer(strategy='mean')\n",
    "    df[numeric_features] = numeric_imputer.fit_transform(df[numeric_features])\n",
    "\n",
    "    binary_imputer = SimpleImputer(strategy='constant', fill_value=0)\n",
    "    df[binary_features] = binary_imputer.fit_transform(df[binary_features])\n",
    "\n",
    "    # Normalize numeric features\n",
    "    scaler = MinMaxScaler()\n",
    "    df[numeric_features] = scaler.fit_transform(df[numeric_features])\n",
    "\n",
    "    # Sort by PID and Subperiod\n",
    "    df = df.sort_values(by=['PID', 'Subperiod']).reset_index(drop=True)\n",
    "\n",
    "    # Group by PID to create sequences\n",
    "    feature_columns = numeric_features + binary_features\n",
    "    grouped = df.groupby('PID')\n",
    "\n",
    "    sequences = []\n",
    "    targets = []\n",
    "    lengths = []\n",
    "\n",
    "    for pid, group in grouped:\n",
    "        # Ensure the group is within the time span\n",
    "        valid_group = group[group['SubperiodLength'].cumsum() <= time_span_days]\n",
    "        if not valid_group.empty:\n",
    "            sequences.append(valid_group[feature_columns].values)\n",
    "            targets.append(valid_group[target_column].iloc[-1])  # Use the last target value\n",
    "            lengths.append(len(valid_group))\n",
    "\n",
    "    # Pad sequences\n",
    "    max_length = max(lengths)\n",
    "    padded_sequences = pad_sequences(sequences, maxlen=max_length, dtype='float32', padding='post', truncating='post')\n",
    "\n",
    "    return padded_sequences, np.array(targets), lengths, feature_columns\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f0cba61-63f9-4ee9-97f5-7395eb553b93",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def preprocess_data(df, time_span_days=ObservationDays, time_resolution_days= increment_days + 1):\n",
    "    # Calculate the number of subperiods for each patient\n",
    "    df['SubperiodLength'] = (df['PeriodEnd'] - df['PeriodStart']).dt.days\n",
    "\n",
    "    # Define the selected features\n",
    "    selected_features = [\n",
    "    'AICDX_I48', 'ATCX_C01E', 'AICDX_F45', 'AICDX_K25', 'AICDX_R93',\n",
    "    'AICDX_M93', 'AICDX_D04', 'ATCX_J01M', 'AICDX_T89', 'ATCX_N02A',\n",
    "    'ATCX_B03A', 'AICDX_I70', 'AICDX_F43', 'ATCX_C08C', 'AICDX_M20',\n",
    "    'AICDX_H19', 'AICDX_I71', 'AICDX_K57', 'AICDX_H35', 'AICDX_I49',\n",
    "    'AICDX_Z96', 'ATCX_H02A', 'AICDX_H18', 'AICDX_H36', 'AICDX_I10',\n",
    "    'AICDX_M84', 'AICDX_G47', 'AICDX_C80', 'AICDX_Q61',\n",
    "    'ATCX_M01A', 'AICDX_M47', 'AICDX_M77', 'AICDX_J38', 'ATCX_C10A',\n",
    "    'AICDX_F17', 'AICDX_E66', 'AICDX_R06', 'AICDX_C50', 'AICDX_R61',\n",
    "    'AICDX_S82', 'ATCX_C09A', 'AICDX_M51', 'AICDX_M81', 'AICDX_F33',\n",
    "    'AICDX_M50', 'AICDX_B99', 'HEMI_9901', 'ATCX_N06A', 'AICDX_R20',\n",
    "    'AICDX_E79', 'AICDX_J06', 'AICDX_J98', 'AICDX_K80', 'HEMI_9701',\n",
    "    'AICDX_G55', 'HEMI_1201', 'AICDX_J20', 'AICDX_I25', 'Sex',\n",
    "    'AICDX_D64', 'AICDX_Z26', 'AICDX_F41', 'AICDX_E04', 'AICDX_I67',\n",
    "    'AICDX_C85', 'AICDX_H25', 'HEMI_501', 'AICDX_U', 'AICDX_M65',\n",
    "    'AICDX_I80', 'AICDX_Z25', 'AICDX_I73', 'AICDX_N28', 'AICDX_Z00',\n",
    "    'ATCX_M03A', 'AICDX_H01', 'HEMI_106', 'HEMI_9933', 'ATCX_A03F',\n",
    "    'AICDX_H43', 'AICDX_T88', 'AICDX_L03', 'AICDX_I65', 'AICDX_T81',\n",
    "    'ATCX_D01A', 'AICDX_M46', 'AICDX_I38', 'ATCX_C01D', 'AICDX_R10',\n",
    "    'AICDX_R26', 'ATCX_B01A', 'AICDX_M42', 'AICDX_R13', 'AICDX_Z24',\n",
    "    'AICDX_R07', 'ATCX_D11A', 'HEMI_8030', 'AICDX_D48',\n",
    "    'AICDX_R47', 'AICDX_Z90', 'AICDX_K40', 'AICDX_R12', 'AICDX_M23',\n",
    "    'AICDX_R51', 'AICDX_A49', 'AICDX_R63', 'AICDX_Z98', 'Age',\n",
    "    'ATCX_G03F', 'ATCX_M03B', 'ATCX_A02B', 'AICDX_F20', 'AICDX_UUU',\n",
    "    'AICDX_N39', 'AICDX_Z13', 'AICDX_M62', 'AICDX_G62', 'AICDX_M16',\n",
    "    'AICDX_Z12', 'ATCX_H03A', 'Treatment_Gap_ATCX', 'AICDX_J39', 'ATCX_J01C',\n",
    "    'ATCX_C03C', 'AICDX_K21', 'ATCX_A06A', 'Treatment_Gap_HEMI', 'Treatment_Gap_AICDX',\n",
    "    'AICDX_Z93', 'AICDX_Z97', 'AICDX_E11', 'AICDX_M48', 'AICDX_F32',\n",
    "    'AICDX_M99', 'AICDX_Z27', 'ATCX_M05B', 'AICDX_I50', 'ATCX_D06A',\n",
    "    'HEMI_201', 'AICDX_E34', 'AICDX_Z76', 'AICDX_H33', 'AICDX_S32',\n",
    "    'AICDX_B00', 'AICDX_M19', 'ATCX_J04A', 'AICDX_K29', 'AICDX_M79',\n",
    "    'AICDX_J32', 'AICDX_L89', 'AICDX_M25', 'AICDX_M54', 'AICDX_N18',\n",
    "    'AICDX_Z47', 'AICDX_D29', 'ATCX_R03B', 'AICDX_R52', 'ATCX_C07A',\n",
    "    'AICDX_H52', 'AICDX_E78', 'AICDX_H40', 'ATCX_P01A', 'AICDX_M53',\n",
    "    'AICDX_T14', 'AICDX_Z01', 'AICDX_H91'\n",
    "]\n",
    "\n",
    "    # Select relevant columns\n",
    "    binary_features = [col for col in df.columns if col.startswith(('ATCX', 'HEMI', 'AICDX'))]\n",
    "    numeric_features = ['Age', 'Treatment_Gap_ATCX', 'Treatment_Gap_HEMI', 'Treatment_Gap_AICDX', 'Sex']\n",
    "    selected_columns = selected_features  # Use only selected features for the model\n",
    "\n",
    "    target_column = '1_Revision'\n",
    "\n",
    "    # Impute missing values\n",
    "    numeric_imputer = SimpleImputer(strategy='mean')\n",
    "    df[numeric_features] = numeric_imputer.fit_transform(df[numeric_features])\n",
    "\n",
    "    binary_imputer = SimpleImputer(strategy='constant', fill_value=0)\n",
    "    df[binary_features] = binary_imputer.fit_transform(df[binary_features])\n",
    "\n",
    "    # Normalize numeric features\n",
    "    scaler = MinMaxScaler()\n",
    "    df[numeric_features] = scaler.fit_transform(df[numeric_features])\n",
    "\n",
    "    # Sort by PID and Subperiod\n",
    "    df = df.sort_values(by=['PID', 'Subperiod']).reset_index(drop=True)\n",
    "\n",
    "    # Group by PID to create sequences\n",
    "    grouped = df.groupby('PID')\n",
    "\n",
    "    sequences = []\n",
    "    targets = []\n",
    "    lengths = []\n",
    "\n",
    "    for pid, group in grouped:\n",
    "        # Ensure the group is within the time span\n",
    "        valid_group = group[group['SubperiodLength'].cumsum() <= time_span_days]\n",
    "        if not valid_group.empty:\n",
    "            sequences.append(valid_group[selected_columns].values)\n",
    "            targets.append(valid_group[target_column].iloc[-1])  # Use the last target value\n",
    "            lengths.append(len(valid_group))\n",
    "\n",
    "    # Pad sequences\n",
    "    max_length = max(lengths)\n",
    "    padded_sequences = pad_sequences(sequences, maxlen=max_length, dtype='float32', padding='post', truncating='post')\n",
    "\n",
    "    return padded_sequences, np.array(targets), lengths, selected_columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "109adc3e-ff41-4961-9dfc-d446414c31b1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Example usage\n",
    "padded_sequences, targets, lengths, feature_columns = preprocess_data(filtered_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44494e16-9d7b-48ff-882d-8eeea231c57c",
   "metadata": {},
   "source": [
    "### LSTM Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2c8d8a3-a8f4-402e-9da6-ac94e4071a16",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch import nn\n",
    "\n",
    "# Define Dataset Class\n",
    "class LSTMFeatureDataset(Dataset):\n",
    "    def __init__(self, sequences, labels):\n",
    "        self.sequences = sequences\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "#Aquí vas tomando la info de paciente a paciente, tanto su secuencia de features, como su target\n",
    "    def __getitem__(self, idx):\n",
    "        return self.sequences[idx], self.labels[idx]\n",
    "\n",
    "# Define LSTM Model\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_layers, output_dim, dropout=0.2):\n",
    "        super(LSTMModel, self).__init__() #Initializes the parent class of LSTMModel, torch.nn.Module\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers = 1, batch_first=True, dropout=dropout)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim) #Defines a fully connected layer for output creation\n",
    "\n",
    "    def forward(self, x):\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        out = self.fc(lstm_out[:, -1, :])  # Use the last hidden state\n",
    "        return out\n",
    "\n",
    "# Split data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(padded_sequences, targets, test_size=0.2, random_state=42)\n",
    "\n",
    "# Prepare Data for PyTorch\n",
    "X_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_tensor = torch.tensor(y_train, dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "train_dataset = LSTMFeatureDataset(X_tensor, y_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# Initialize Model, Loss, and Optimizer\n",
    "input_dim = X_train.shape[2]\n",
    "hidden_dim = 64\n",
    "num_layers = 2\n",
    "output_dim = 1\n",
    "dropout = 0.7\n",
    "\n",
    "model = LSTMModel(input_dim, hidden_dim, num_layers, output_dim, dropout)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0005, weight_decay=.005)\n",
    "\n",
    "# Train LSTM Model\n",
    "num_epochs = 10\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_loss = 0\n",
    "    for sequences, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(sequences)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {epoch_loss:.4f}\")\n",
    "\n",
    "# Evaluate Model on Train Set\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    train_outputs = model(X_tensor)\n",
    "    train_preds = torch.sigmoid(train_outputs).numpy()\n",
    "    auc_score = roc_auc_score(y_train, train_preds)\n",
    "    print(f\"Train AUC: {auc_score:.4f}\")\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    test_outputs = model(X_test_tensor)\n",
    "    test_preds = torch.sigmoid(test_outputs).numpy()\n",
    "    test_auc = roc_auc_score(y_test, test_preds)\n",
    "    print(f\"Test AUC: {test_auc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e06362c-1ab2-4080-9679-3cd4cfb9b5d0",
   "metadata": {},
   "source": [
    "### Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d64693bc-6776-4a80-a760-e3d54a97d614",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from hyperopt import hp, fmin, tpe, Trials\n",
    "\n",
    "# Objective function for Bayesian Optimization\n",
    "def objective(params):\n",
    "    hidden_dim = int(params['hidden_dim'])  # Hyperparameter: number of units in the hidden layer\n",
    "    num_layers = int(params['num_layers'])  # Hyperparameter: number of LSTM layers\n",
    "    dropout = params['dropout']  # Hyperparameter: dropout rate\n",
    "    lr = params['lr']  # Hyperparameter: learning rate\n",
    "    \n",
    "    model = LSTMModel(input_dim=X_train.shape[2], hidden_dim=hidden_dim, \n",
    "                      num_layers=num_layers, output_dim=1, dropout=dropout)\n",
    "    \n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    \n",
    "    # Train the model\n",
    "    model.train()\n",
    "    num_epochs = 10\n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_loss = 0\n",
    "        for sequences, labels in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(sequences)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "    # Evaluate on train set\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        train_outputs = model(X_tensor)\n",
    "        train_preds = torch.sigmoid(train_outputs).numpy()\n",
    "        auc_score = roc_auc_score(y_train, train_preds)\n",
    "        \n",
    "    return -auc_score  # Return negative AUC to minimize the objective function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e88e49cf-c24c-46f9-9432-691751ba1674",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the hyperparameter search space\n",
    "space = {\n",
    "    'hidden_dim': hp.quniform('hidden_dim', 16, 48, 16),  # Hidden dimension size\n",
    "    'num_layers': hp.quniform('num_layers', 1, 2, 1),  # Number of LSTM layers\n",
    "    'dropout': hp.uniform('dropout', 0.35, 0.7),  # Dropout rate\n",
    "    'lr': hp.loguniform('lr', -10, -5.3),  # Learning rate (log-uniform)\n",
    "    'weight_decay': hp.loguniform('weight_decay', -6, -3)  # Add weight regularization\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75a6d8db-017c-4ae6-aa24-e8fb54678f6e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Initialize Trials object to store optimization results\n",
    "trials = Trials()\n",
    "\n",
    "# Run the optimization\n",
    "best = fmin(fn=objective, space=space, algo=tpe.suggest, max_evals=20, trials=trials)\n",
    "\n",
    "# Output the best hyperparameters found\n",
    "print(\"Best hyperparameters:\", best)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11f012d2-c6ec-4db3-a351-e09595f4190f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "best_params = best\n",
    " \n",
    "# Retrieve the best hyperparameters\n",
    "hidden_dim_best = int(best_params['hidden_dim'])\n",
    "num_layers_best = int(best_params['num_layers'])\n",
    "dropout_best = best_params['dropout']\n",
    "lr_best = best_params['lr']\n",
    "\n",
    "# Retrain the model with the best hyperparameters\n",
    "model = LSTMModel(input_dim=X_train.shape[2], hidden_dim=hidden_dim_best, \n",
    "                  num_layers=num_layers_best, output_dim=1, dropout=dropout_best)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr_best)\n",
    "\n",
    "# Train the model again (you can use the same code as before)\n",
    "# Train the model\n",
    "model.train()\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_loss = 0\n",
    "    for sequences, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(sequences)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "# Evaluate and print the final AUC score on the test set\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    test_outputs = model(X_test_tensor)\n",
    "    test_preds = torch.sigmoid(test_outputs).numpy()\n",
    "    test_auc = roc_auc_score(y_test, test_preds)\n",
    "    print(f\"Final Test AUC: {test_auc:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
